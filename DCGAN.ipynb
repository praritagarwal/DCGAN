{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project to build and train a DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.1.0\n",
      "keras version: 2.2.4-tf\n",
      "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))\n",
    "print('GPU: {}'.format(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in this project, we ultimately wish to train the GAN on the [lfw](https://www.tensorflow.org/datasets/catalog/lfw) dataset to make it generate human faces, we will start small and train it on the Fashion-MNIST for practice. Once that works, we will try to build upon our acquired knowledge to train on the lfw dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train, _), (_,_) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train: (60000, 28, 28)\n",
      "max pixel value: 255\n",
      "min pixel value: 0\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train: {}'.format(X_train.shape))\n",
    "print('max pixel value: {}'.format(X_train.max()))\n",
    "print('min pixel value: {}'.format(X_train.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new pixel max: 1.0\n",
      "new pixel min: -1.0\n"
     ]
    }
   ],
   "source": [
    "# scale the pixel value to have max = 1 and min = -1\n",
    "max_px = X_train.max()\n",
    "min_px = X_train.min()\n",
    "A = 2/(max_px - min_px)\n",
    "B = -1*(max_px + min_px)/(max_px - min_px)\n",
    "X_train = A*X_train + B\n",
    "print('new pixel max: {}'.format(X_train.max()))\n",
    "print('new pixel min: {}'.format(X_train.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Reshape, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a generator of given specifications\n",
    "# Assumption: the first layer is a dense layer and \n",
    "# the rest are transpose Convolutional layers\n",
    "# list layers consists of a list of tuples except for \n",
    "# the first entry which is a tuple: (num_units, activation) for the dense layer\n",
    "# the second entry is tuple of two objects: the height and the width of \n",
    "# the feature maps obtained from reshaping the output of the first layer\n",
    "# the rest of the enteries are tuples: (num_units, kernel_size, stride, padding, activation) for\n",
    "# the transpose convolutional layers\n",
    "# if batch_normalization == True, a BatchNormalization layer will be included after \n",
    "# every hidden layer except the first dense layer\n",
    "# if batch_normalization == False and drop_prob!=0, then \n",
    "# a dropout layer is added after every layer except the dense layer\n",
    "def build_generator(list_layers, coding_size = 100, batch_normalization = True, drop_prob = 0):\n",
    "    \n",
    "    dense_units, dense_act = list_layers.pop(0)\n",
    "    reshape_x, reshape_y = list_layers.pop(0)\n",
    "    reshape_channels = int(dense_units/(reshape_x*reshape_y))\n",
    "    \n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(units=dense_units, activation = dense_act, \n",
    "                        input_shape=[coding_size]))\n",
    "    generator.add(Reshape([reshape_x, reshape_y, reshape_channels]))\n",
    "    \n",
    "    for layer in list_layers:\n",
    "        if batch_normalization:\n",
    "            generator.add(BatchNormalization())\n",
    "        elif drop_prob!=0:\n",
    "            generator.add(Dropout(drop_prob))\n",
    "            \n",
    "        num_units, kernel_size, stride, padding, activation = layer\n",
    "        trans_conv = Conv2DTranspose(filters = num_units, kernel_size = kernel_size, \n",
    "                                     strides = stride, padding = padding, \n",
    "                                     activation = activation)\n",
    "        generator.add(trans_conv)\n",
    "        \n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 1)         1601      \n",
      "=================================================================\n",
      "Total params: 840,705\n",
      "Trainable params: 840,321\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l1 = (7*7*128, 'selu')\n",
    "l2 = (7, 7)\n",
    "l3 = (64, 5, 2, 'same', 'selu')\n",
    "l4 = (1, 5, 2, 'same', 'tanh')\n",
    "\n",
    "list_layers = [l1,l2,l3,l4]\n",
    "generator = build_generator(list_layers)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a generator with given specifications\n",
    "# list_layers consists of a list of tuples used to describe each convolutional layer\n",
    "# tuples: (num_units, kernel_size, stride, padding, activation)\n",
    "# if batch_normalization is True, then each layer is followed by a BatchNormalization layer\n",
    "# if batch_normalization is False and drop_prob!=0, then each layer is followed by a dropout layer\n",
    "def build_discriminator(list_layers, input_shape, batch_normalization = False, drop_prob = 0.3):\n",
    "    \n",
    "    discriminator = Sequential()\n",
    "    filters, kernel_size, stride, padding, activation = list_layers.pop(0) \n",
    "    conv2d = Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                    strides = stride, padding = padding, \n",
    "                    activation = activation, input_shape = input_shape)\n",
    "    discriminator.add(conv2d)\n",
    "    \n",
    "    for filters, kernel_size, stride, padding, activation  in list_layers:\n",
    "        if batch_normalization:\n",
    "            discriminator.add(BatchNormalization())\n",
    "        elif drop_prob!=0:\n",
    "            discriminator.add(Dropout(drop_prob))\n",
    "        \n",
    "        conv2d = Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                    strides = stride, padding = padding, activation = activation, )\n",
    "        \n",
    "        discriminator.add(conv2d)\n",
    "        \n",
    "        discriminator.add(Flatten())\n",
    "        discriminator.add(Dense(1, activation = 'sigmoid'))\n",
    "        \n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l1 = (64, 5, 2, 'same', keras.layers.LeakyReLU(0.2))\n",
    "l2 = (128, 5, 2, 'same', keras.layers.LeakyReLU(0.2))\n",
    "list_layers = [l1,l2]\n",
    "input_shape = [28,28,1]\n",
    "discriminator = build_discriminator(list_layers, input_shape)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine the generator and the discriminator into a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gan = Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the discriminator\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'rmsprop'\n",
    "metrics = ['acc']\n",
    "discriminator.compile(loss = loss, optimizer = optimizer, metrics = metrics )\n",
    "discriminator.trainable = False\n",
    "Gan.compile(loss = loss, optimizer = optimizer, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop for the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the gan\n",
    "def train_gan(gan, training_images, num_epochs = 2, batch_size = 32, coding_size = 100):\n",
    "    \n",
    "    generator, discriminator = gan.layers\n",
    "    \n",
    "    idxs = tf.range(2*batch_size)\n",
    "    for epoch in tf.range(num_epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        for true_imgs in training_images:\n",
    "            \n",
    "            # generate a set of random codings\n",
    "            codings = tf.random.normal(shape = (batch_size, coding_size))\n",
    "            # convert the codings into images\n",
    "            gan_imgs = generator(codings)\n",
    "            y_gan = tf.constant([[0.]]*batch_size)\n",
    "            \n",
    "            # mix the gan images with the true images\n",
    "            y_true = tf.constant([[1.]]*batch_size)\n",
    "            all_imgs = tf.concat([true_imgs, gan_imgs], axis = 0)\n",
    "            all_y = tf.concat([y_true, y_gan], axis = 0)\n",
    "            \n",
    "            # shuffle the set of mixed images\n",
    "            shuffled_idxs = tf.random.shuffle(idxs)\n",
    "            shuffled_imgs = tf.gather(all_imgs, shuffled_idxs)\n",
    "            shuffled_y = tf.gather(all_y, shuffled_idxs)\n",
    "            \n",
    "            # phase 1: train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(shuffled_imgs, shuffled_y)\n",
    "        \n",
    "            # phase 2: train the generator on a new set of codings\n",
    "            new_codings = tf.random.normal(shape = (batch_size, coding_size))\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(codings, 1-y_gan)\n",
    "        \n",
    "        end = time.time()\n",
    "        time_taken = end - start\n",
    "        print('\\repoch:{0:}/{1:}, time:{2: .2f}s '.format(epoch+1, num_epochs, time_taken), \n",
    "              end = '', flush = True)\n",
    "        print('=', end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a batched dataset from X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the type of X_train to float32\n",
    "X_train = tf.cast(X_train, tf.float32)\n",
    "# Add a channel dimension to X_train\n",
    "X_train = tf.reshape(X_train, [-1,28,28,1])\n",
    "# create a dataset\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "training_images = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size = buffer_size)\n",
    "training_images = training_images.batch(batch_size).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50/50, time: 25.22s ================================================="
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "coding_size = 100\n",
    "train_gan(Gan, training_images, num_epochs, batch_size, coding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
